{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import libs.commons as commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = Path(commons.dataset_path) / \"train.csv\"\n",
    "test_path = Path(commons.dataset_path) / \"test.csv\"\n",
    "test_set = pd.read_csv(test_path)\n",
    "train_set = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text_len'] = train_set['text'].apply(len)"
   ]
  },
  {
   "source": [
    "Taking a look at the dataset by examining the first few rows of train and test sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  text_len  \n",
       "0       1        69  \n",
       "1       1        38  \n",
       "2       1       133  \n",
       "3       1        65  \n",
       "4       1        88  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>69</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>133</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>88</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "source": [
    "Let's examine the keyword feature by sorting the data by the most frequently ocurring keywords"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "221 unique keywords\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                       id  location  text  target  text_len\n",
       "keyword                                                    \n",
       "fatalities             45        32    45      45        45\n",
       "deluge                 42        29    42      42        42\n",
       "armageddon             42        32    42      42        42\n",
       "sinking                41        23    41      41        41\n",
       "damage                 41        30    41      41        41\n",
       "...                    ..       ...   ...     ...       ...\n",
       "forest%20fire          19        12    19      19        19\n",
       "epicentre              12         9    12      12        12\n",
       "threat                 11        10    11      11        11\n",
       "inundation             10         5    10      10        10\n",
       "radiation%20emergency   9         6     9       9         9\n",
       "\n",
       "[221 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text_len</th>\n    </tr>\n    <tr>\n      <th>keyword</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>fatalities</th>\n      <td>45</td>\n      <td>32</td>\n      <td>45</td>\n      <td>45</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>deluge</th>\n      <td>42</td>\n      <td>29</td>\n      <td>42</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>armageddon</th>\n      <td>42</td>\n      <td>32</td>\n      <td>42</td>\n      <td>42</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>sinking</th>\n      <td>41</td>\n      <td>23</td>\n      <td>41</td>\n      <td>41</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>damage</th>\n      <td>41</td>\n      <td>30</td>\n      <td>41</td>\n      <td>41</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>forest%20fire</th>\n      <td>19</td>\n      <td>12</td>\n      <td>19</td>\n      <td>19</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>epicentre</th>\n      <td>12</td>\n      <td>9</td>\n      <td>12</td>\n      <td>12</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>threat</th>\n      <td>11</td>\n      <td>10</td>\n      <td>11</td>\n      <td>11</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>inundation</th>\n      <td>10</td>\n      <td>5</td>\n      <td>10</td>\n      <td>10</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>radiation%20emergency</th>\n      <td>9</td>\n      <td>6</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>221 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "## Frequency of Keyword values\n",
    "# train_set.describe()\n",
    "print(f\"{train_set['keyword'].nunique()} unique keywords\")\n",
    "key_group = train_set.groupby('keyword')\n",
    "key_group.count().sort_values(by='id', ascending=False)"
   ]
  },
  {
   "source": [
    "The keywords feature seems reasonable. We can see a few Unicode space symbols (%20), but the rarest keyword still has 9 occurrences.\n",
    "Let's do the same to the location feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 id  keyword  text  target  text_len\n",
       "location                                            \n",
       "USA             104      104   104     104       104\n",
       "New York         71       71    71      71        71\n",
       "United States    50       50    50      50        50\n",
       "London           45       45    45      45        45\n",
       "Canada           29       29    29      29        29\n",
       "...             ...      ...   ...     ...       ...\n",
       "Hueco Mundo       1        1     1       1         1\n",
       "Hughes, AR        1        1     1       1         1\n",
       "Huntington, WV    1        1     1       1         1\n",
       "Huntley, IL       1        1     1       1         1\n",
       "åø\\_(?)_/åø       1        1     1       1         1\n",
       "\n",
       "[3341 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text_len</th>\n    </tr>\n    <tr>\n      <th>location</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>USA</th>\n      <td>104</td>\n      <td>104</td>\n      <td>104</td>\n      <td>104</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>New York</th>\n      <td>71</td>\n      <td>71</td>\n      <td>71</td>\n      <td>71</td>\n      <td>71</td>\n    </tr>\n    <tr>\n      <th>United States</th>\n      <td>50</td>\n      <td>50</td>\n      <td>50</td>\n      <td>50</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>London</th>\n      <td>45</td>\n      <td>45</td>\n      <td>45</td>\n      <td>45</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>Canada</th>\n      <td>29</td>\n      <td>29</td>\n      <td>29</td>\n      <td>29</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>Hueco Mundo</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Hughes, AR</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Huntington, WV</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Huntley, IL</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>åø\\_(?)_/åø</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3341 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "## Frequency of location values\n",
    "loc_group = train_set.groupby('location')\n",
    "loc_group.count().sort_values(by='id', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                 id  keyword  location  text  target  text_len\n",
       "location_clean                                                                \n",
       "nan                            2533     2472         0  2533    2533      2533\n",
       "usa                             104      104       104   104     104       104\n",
       "new york                         75       75        75    75      75        75\n",
       "united states                    50       50        50    50      50        50\n",
       "london                           49       49        49    49      49        49\n",
       "...                             ...      ...       ...   ...     ...       ...\n",
       "haiku, maui, hawaii               1        1         1     1       1         1\n",
       "hailing from dayton               1        1         1     1       1         1\n",
       "halfrica                          1        1         1     1       1         1\n",
       "halifax, nouvelle-ìäcosse         1        1         1     1       1         1\n",
       "ìøåàå_t: 40.736324,-73.990062     1        1         1     1       1         1\n",
       "\n",
       "[3234 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>text_len</th>\n    </tr>\n    <tr>\n      <th>location_clean</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>nan</th>\n      <td>2533</td>\n      <td>2472</td>\n      <td>0</td>\n      <td>2533</td>\n      <td>2533</td>\n      <td>2533</td>\n    </tr>\n    <tr>\n      <th>usa</th>\n      <td>104</td>\n      <td>104</td>\n      <td>104</td>\n      <td>104</td>\n      <td>104</td>\n      <td>104</td>\n    </tr>\n    <tr>\n      <th>new york</th>\n      <td>75</td>\n      <td>75</td>\n      <td>75</td>\n      <td>75</td>\n      <td>75</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>united states</th>\n      <td>50</td>\n      <td>50</td>\n      <td>50</td>\n      <td>50</td>\n      <td>50</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>london</th>\n      <td>49</td>\n      <td>49</td>\n      <td>49</td>\n      <td>49</td>\n      <td>49</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>haiku, maui, hawaii</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>hailing from dayton</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>halfrica</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>halifax, nouvelle-ìäcosse</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>ìøåàå_t: 40.736324,-73.990062</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3234 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "def clean_loc(x):\n",
    "    x = str(x)\n",
    "    if x:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "train_proc = train_set.copy()\n",
    "train_proc['location_clean'] = train_proc['location'].apply(clean_loc)\n",
    "loc_group = train_proc.groupby('location_clean')\n",
    "loc_group.count().sort_values(by='id', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "thern ireland\n",
      "northern kentucky, usa\n",
      "norwalk, ct\n",
      "norway\n",
      "norwich\n",
      "not a u.s resident\n",
      "not los angeles, not new york.\n",
      "not of this world\n",
      "not so cool ky\n",
      "not steven yeun / amc.\n",
      "not where i want to be, yet\n",
      "nottingham\n",
      "nottingham, england\n",
      "nottingham, united kingdom\n",
      "notts\n",
      "nova scotia, canada\n",
      "novi, mi\n",
      "nowhere\n",
      "nowhere islands/smash manor\n",
      "nowhere. everywhere.\n",
      "numa casa de old yellow bricks\n",
      "numenor\n",
      "nunya\n",
      "nv\n",
      "ny\n",
      "ny capital district\n",
      "ny || live easy? \n",
      "ny, ct & greece\n",
      "ny, ny\n",
      "nyc\n",
      "nyc / international\n",
      "nyc :) ex- #islamophobe\n",
      "nyc area\n",
      "nyc metro\n",
      "nyc&nj\n",
      "nyc, new york\n",
      "nyc,us - cali, colombia\n",
      "nyc-la-miami\n",
      "nyhc\n",
      "oakland\n",
      "oakland, ca\n",
      "oblivion?\n",
      "ocean city, nj\n",
      "odawara, japan\n",
      "oes 4th point. sisstar & ti\n",
      "official website\n",
      "ogba, lagos, nigeria\n",
      "ohio\n",
      "ohio, usa\n",
      "ojodu,lagos\n",
      "ok\n",
      "okanagan valley, bc\n",
      "oklahoma\n",
      "oklahoma city\n",
      "oklahoma city, ok\n",
      "oklahoma, usa\n",
      "okuma town, fukushima\n",
      "olathe, ks\n",
      "old blighty\n",
      "oldenburg // london\n",
      "olympia, wa\n",
      "oman muscat al seeb \n",
      "on\n",
      "on a beach \n",
      "on a catwalk somewhere\n",
      "on the court \n",
      "on the go\n",
      "on the toilet having a dump \n",
      "on the web\n",
      "on to the next adventure\n",
      "on twitter\n",
      "on twitter \n",
      "ona block w/ my boy ??\n",
      "ondo\n",
      "one world\n",
      "oneonta, ny/ staten island, ny\n",
      "online \n",
      "online 24/7. not even kidding.\n",
      "ontario\n",
      "ontario canada\n",
      "ontario, canada\n",
      "ontario, canada. \n",
      "orange county, ca\n",
      "orange county, calif.\n",
      "orange county, california\n",
      "orange county, ny\n",
      "orbost, victoria, australia\n",
      "oregon\n",
      "oregon and washington\n",
      "oregon, usa\n",
      "orlando\n",
      "orlando \n",
      "orlando, fl\n",
      "orlando,fl  usa\n",
      "orlando/cocoa beach, fl\n",
      "orm\n",
      "ormond by the sea, fl\n",
      "oshawa, canada\n",
      "oshawa/toronto\n",
      "oslo, norway\n",
      "otsego, mi\n",
      "ottawa, canada\n",
      "ottawa, ontario\n",
      "ottawa,ontario canada\n",
      "our empire state\n",
      "outside the matrix, i think.\n",
      "over the moon...\n",
      "overland park, ks\n",
      "overton nv\n",
      "own planet!!\n",
      "oxford\n",
      "oxford / bristol\n",
      "oxford, ms\n",
      "oxford, oh\n",
      "pa\n",
      "pa, usa\n",
      "pa.usa\n",
      "pacific northwest\n",
      "paducah, ky\n",
      "paignton\n",
      "pakistan\n",
      "pakistan, islamabad\n",
      "palermo, sicily\n",
      "palestine \n",
      "palestine texas\n",
      "palm bay, fl (kissimmee)\n",
      "palm beach county, fl\n",
      "palm desert, ca\n",
      "palma, islas baleares\n",
      "palmyra, nj\n",
      "palo alto, ca\n",
      "palo alto, california\n",
      "paname city\n",
      "panamìá \n",
      "paonia, colorado \n",
      "parachute\n",
      "paradise\n",
      "paradise city\n",
      "paradise, nv\n",
      "paranaque city\n",
      "paris\n",
      "paris \n",
      "paris (france)\n",
      "paris, france\n",
      "paris.\n",
      "park ridge, illinois\n",
      "passamaquoddy\n",
      "paterson, new jersey \n",
      "patra-greece.\n",
      "paulton, england\n",
      "pawnee\n",
      "pdx\n",
      "pedophile hunting ground\n",
      "peekskill. new york, 10566 \n",
      "pekanbaruå¡batam islandå¡medan\n",
      "pelham, al\n",
      "pembroke nh\n",
      "penn hills, pa\n",
      "pennsylvania\n",
      "pennsylvania, pa\n",
      "pennsylvania, usa\n",
      "pensacola, fl\n",
      "peoria\n",
      "perenjori, wa\n",
      "perth, australia\n",
      "perth, australia \n",
      "perth, western australia\n",
      "perthshire \n",
      "peru\n",
      "peshawar\n",
      "peshawar pakistan \n",
      "petaluma, ca\n",
      "peterborough, on\n",
      "peterborough, ont.\n",
      "peterborough, ontario, canada\n",
      "pettyville, usa\n",
      "pg chillin!\n",
      "pg county, md\n",
      "ph\n",
      "phila.\n",
      "philadelphia\n",
      "philadelphia, pa\n",
      "philadelphia, pa \n",
      "philadelphia, pa usa\n",
      "philadelphia, pennsylvania\n",
      "philadelphia, pennsylvania usa\n",
      "philippines\n",
      "philippines \n",
      "philly\n",
      "philly \n",
      "phoenix\n",
      "phoenix az\n",
      "phoenix, arizona, usa\n",
      "phoenix, az\n",
      "photo : blue mountains \n",
      "phuket thailand\n",
      "piedmont area, north carolina\n",
      "piedmont triad, nc\n",
      "pig symbol, alabama\n",
      "pioneer village, ky\n",
      "pissing off antis\n",
      "pittsboro\n",
      "pittsburgh\n",
      "pittsburgh \n",
      "pittsburgh pa\n",
      "plain o' texas\n",
      "planet earth\n",
      "planet eyal, shandral system\n",
      "planet of da bathing apes\n",
      "planeta h2o\n",
      "plano, il\n",
      "plano, texas\n",
      "plano,tx\n",
      "play for ryan ??\n",
      "playa\n",
      "playa del carmen, mexico\n",
      "playing soccer & eating pizza\n",
      "pleasanton, ca\n",
      "please h? ?:??\n",
      "plfd cuh..\n",
      "pluto\n",
      "plymouth\n",
      "pocatello, id\n",
      "pocatello, idaho\n",
      "poconos\n",
      "poffin\n",
      "polmont \n",
      "pomfret/providence\n",
      "pompano beach, fl\n",
      "pon di gully\n",
      "pontefract uk\n",
      "pontevedra, galicia\n",
      "poplar, london\n",
      "port charlotte, fl\n",
      "port harcourt, nigeria\n",
      "port jervis, ny\n",
      "port matilda pa\n",
      "port orange, fl\n",
      "port williams ns\n",
      "portage, in / worldwide\n",
      "porthcawl\n",
      "portland, or\n",
      "portland, ore. \n",
      "portland, oregon\n",
      "porto alegre, rio grande do sul\n",
      "portoviejo-manabi-ecuador\n",
      "portsmouth, uk\n",
      "portsmouth, va\n",
      "portugal\n",
      "positive 852\n",
      "potters bar\n",
      "pratt-on-wye\n",
      "predjama, eslovenia.\n",
      "prehistoric earth\n",
      "pretoria\n",
      "primum non nocere\n",
      "principality of zeron\n",
      "pro-american and anti-#occupy\n",
      "prob turning up with sheen\n",
      "probably not home\n",
      "probably petting an animal\n",
      "probably the strip club\n",
      "probably watching survivor\n",
      "proud @buckmasonusa supporter!\n",
      "proud indians\n",
      "proudly canadian!\n",
      "proudly frozen canuck eh !!\n",
      "proudly south african\n",
      "prov\n",
      "providence ri / lisnaskea \n",
      "ps4, now stop asking\n",
      "psa nursing \n",
      "psn: pipbois \n",
      "pueblo, co\n",
      "pueblo, colorado\n",
      "puerto rico\n",
      "pune, maharashtra\n",
      "pune, mostly \n",
      "punjab\n",
      "punpunlì¢ndia\n",
      "purfleet\n",
      "purgatory, usa\n",
      "purple booth studioã¢\n",
      "qld australia\n",
      "qosqo\n",
      "quantico, va\n",
      "queen creek az\n",
      "queens new york\n",
      "queens, ny\n",
      "queens.\n",
      "queensland\n",
      "queensland, australia\n",
      "quezon city, philippines\n",
      "quilmes , arg\n",
      "quincy\n",
      "quincy ma\n",
      "quito, ecuador.\n",
      "r'lyeh, south pacific\n",
      "rafael castillo\n",
      "raleigh (garner/cleveland) nc\n",
      "raleigh durham, nc\n",
      "raleigh, nc\n",
      "rapid city, black hills, sd\n",
      "rapid city, south dakota\n",
      "reading a romance novel\n",
      "reading ma\n",
      "reading uk\n",
      "reality\n",
      "realville\n",
      "redding, california, usa\n",
      "reddit\n",
      "reddit \n",
      "redondo beach, ca\n",
      "regalo island\n",
      "renfrew, scotland\n",
      "republic of texas\n",
      "republic of the philippines\n",
      "republica dominicana\n",
      "reston, va, usa\n",
      "rheinbach / germany\n",
      "rhode island\n",
      "rhodeisland\n",
      "rhyme or reason?\n",
      "richardson tx\n",
      "richmond heights, oh\n",
      "richmond, va\n",
      "right here\n",
      "right next to compton\n",
      "right next to you\n",
      "rio\n",
      "rio de janeiro\n",
      "rio de janeiro | brazil\n",
      "riverdale, ga \n",
      "riverside, ca\n",
      "riverside, california.\n",
      "riverview, fl \n",
      "riyadh\n",
      "riyadh ')\n",
      "roads/trails everywhere\n",
      "roadside\n",
      "roaming around the world\n",
      "roanoke va\n",
      "roanoke, va\n",
      "robin hood's county \n",
      "rochelle, ga\n",
      "rochester\n",
      "rochester hills, mi\n",
      "rochester, ny\n",
      "rock hill, sc\n",
      "rock springs, wy\n",
      "rocketing through the galaxy\n",
      "rockford, il\n",
      "rockland county, ny\n",
      "rockville, maryland\n",
      "rocky mountains\n",
      "rogersville, mo\n",
      "romania\n",
      "rome\n",
      "rome, italy\n",
      "room 234\n",
      "roppongi, minato, tokyo \n",
      "rotterdam, the netherlands\n",
      "rotterdam, zuid-holland\n",
      "rowyso dallas \n",
      "royton\n",
      "rsn: tru\n",
      "rural northern nevada\n",
      "rural ohio (fuck)\n",
      "russia\n",
      "rutherfordton, nc\n",
      "rzl ?\n",
      "s.f. bay area\n",
      "sacae plains\n",
      "sacramento\n",
      "sacramento, ca\n",
      "sacramento, california\n",
      "saint louis, missouri\n",
      "saint lucia\n",
      "saint marys, ga\n",
      "saint paul\n",
      "saipan, cnmi\n",
      "sale, england\n",
      "saline, mi\n",
      "salt lake city, ut\n",
      "salt lake city, utah\n",
      "saltillo, coahuila de zaragoza\n",
      "san antonio, tx\n",
      "san antonio-ish, tx\n",
      "san diego\n",
      "san diego ca\n",
      "san diego california 92101\n",
      "san diego, ca\n",
      "san diego, calif.\n",
      "san diego, california\n",
      "san diego, texas.\n",
      "san francisco\n",
      "san francisco , ca\n",
      "san francisco bay area\n",
      "san francisco, ca\n",
      "san fransokyo\n",
      "san gabriel la union\n",
      "san jose\n",
      "san jose, ca\n",
      "san jose, ca, usa\n",
      "san jose, california\n",
      "san juan, puerto rico\n",
      "san luis obispo, ca\n",
      "san mateo county, ca\n",
      "sand springs oklahoma\n",
      "sandton, south africa\n",
      "sanganer, rajasthan\n",
      "santa clara, ca\n",
      "santa cruz, ca\n",
      "santa maria, ca\n",
      "santa monica, ca\n",
      "santiago bernabeau\n",
      "santiago de chile\n",
      "santiago de cmpostela galicia\n",
      "santiago,repì¼blica dominicana\n",
      "santo domingo\n",
      "santo domingo alma rosa \n",
      "sao paulo\n",
      "sao paulo, brazil\n",
      "sarasota, fl\n",
      "saskatchewan, canada\n",
      "satan's colon\n",
      "saudi arabia\n",
      "saudi arabia - riyadh \n",
      "savage states of america\n",
      "savannah, ga\n",
      "scandinavia\n",
      "scituate, ma\n",
      "score more goals buying @\n",
      "score team goals buying @\n",
      "scotland\n",
      "scotland \n",
      "scotland, united kingdom\n",
      "scotts valley, ca\n",
      "scottsdale, az\n",
      "scottsdale. az\n",
      "screwston, tx\n",
      "scumbernauld\n",
      "sd |norway| ksa\n",
      "se london(heart is by the sea)\n",
      "sea server\n",
      "searching for bae \n",
      "seattle\n",
      "seattle grace mercy death\n",
      "seattle native in prescott, az\n",
      "seattle wa\n",
      "seattle, wa\n",
      "seattle, wa usa\n",
      "seattle, washington\n",
      "sec country\n",
      "see the barn of bleakness\n",
      "selangor\n",
      "selena | britney | hilary\n",
      "selma2oakland\n",
      "semarang, indonesia\n",
      "seoul\n",
      "serva fidem\n",
      "sevier county.\n",
      "sf bay area\n",
      "sf bay area, california / greater phoenix, az\n",
      "shady pines \n",
      "shah alam,malaysia\n",
      "shanghai\n",
      "sharkatraz/bindle's cleft, pa\n",
      "she/her/your majesty/empress\n",
      "sheff/bangor/salamanca/madrid\n",
      "sheffield // rotherham\n",
      "sheffield township, ohio\n",
      "sheffield/leeds\n",
      "shelby county\n",
      "sherwood, brisbane, australia\n",
      "shipwreck cove\n",
      "shirley, ny\n",
      "shity land of northern ireland\n",
      "shoujo hell \n",
      "shrewsbury\n",
      "sicamous, british columbia\n",
      "silang, cavite / paraì±aque\n",
      "silesia, poland\n",
      "silicon valley\n",
      "silver spring, md\n",
      "silvermoon or ironforge\n",
      "sindria\n",
      "singapore\n",
      "sioux falls, s.d. \n",
      "sioux falls, sd\n",
      "sisterhood\n",
      "sitting on eddie vedders lap,\n",
      "sitting on the fence, new york\n",
      "skyhold\n",
      "skyport de la rosa\n",
      "slappin and smackin \n",
      "slateport city, hoenn\n",
      "slatina,romania\n",
      "sligo and galway, ireland\n",
      "smash manor/kanto\n",
      "snapchat~ maddzz_babby \n",
      "sneaking glances at thancred\n",
      "socal\n",
      "sochi, kda, ru\n",
      "sodak\n",
      "somalia\n",
      "some other mansion\n",
      "some pum pum\n",
      "some where\n",
      "some where in this world\n",
      "somecity, somerset, md\n",
      "someday i'll live in england. \n",
      "someplace living my life\n",
      "somerset, uk\n",
      "somewhere\n",
      "somewhere \n",
      "somewhere around you\n",
      "somewhere between chicago & milwaukee\n",
      "somewhere between here & there\n",
      "somewhere else...\n",
      "somewhere in cali \n",
      "somewhere in china.\n",
      "somewhere in indiana \n",
      "somewhere in jersey\n",
      "somewhere in portugal\n",
      "somewhere in spain\n",
      "somewhere in the canada\n",
      "somewhere only we know ?\n",
      "somewhere out there\n",
      "somewhere outside\n",
      "somewhere over a rainbow\n",
      "somewhere powerbraking a chevy\n",
      "somewhere too cold for me\n",
      "somewhere usa \n",
      "somewhere with clyde\n",
      "soufside\n",
      "soul somalia/body montreal\n",
      "south 37\n",
      "south africa\n",
      "south africa eastern cape\n",
      "south asia\n",
      "south bloomfield, oh\n",
      "south carolina\n",
      "south carolina, usa\n",
      "south central wales\n",
      "south east of u.k\n",
      "south florida\n",
      "south korea gmt+9\n",
      "south of d.c.\n",
      "south of heaven \n",
      "south pasadena, ca\n",
      "south stand\n",
      "south west, england\n",
      "south, england\n",
      "south, usa\n",
      "southeast asia\n",
      "southern califorina\n",
      "southern california\n",
      "southern california desert\n",
      "southern maine\n",
      "southwest, tx\n",
      "spain\n",
      "spain - china - latin america.\n",
      "spain but opa-locka, fl\n",
      "spare 'oom\n",
      "speaking the truth in love\n",
      "spinning through time.\n",
      "spokane, wa\n",
      "spokane, washington\n",
      "spokane, washington 99206\n",
      "spring grove, il\n",
      "spring tx\n",
      "spying on your thoughts\n",
      "sri lanka\n",
      "srinagar,kashmir\n",
      "ss\n",
      "st austell, cornwall\n",
      "st charles, md\n",
      "st joseph de beauce\n",
      "st louis, mo\n",
      "st paul, mn\n",
      "st petersburgfl\n",
      "st. catharines, ontario\n",
      "st. john's, nl, canada\n",
      "st. joseph, minnesota\n",
      "st. louis\n",
      "st. louis mo.\n",
      "st. louis, missouri\n",
      "st. louis, mo\n",
      "st. patrick's purgatory\n",
      "st.cloud, mn\n",
      "st.louis county missouri \n",
      "stage with trey songz\n",
      "staggering on tenement roofs\n",
      "stalybridge, tameside\n",
      "stamford & cork (& shropshire)\n",
      "stanford university\n",
      "starling city\n",
      "state college, pa\n",
      "state of dreaming\n",
      "state of georgia\n",
      "stateless global citizen\n",
      "statesboro/vidalia\n",
      "statute of limitations_\n",
      "stay fly?\n",
      "stay tuned ;) \n",
      "still. ??s.a.n.d.o.s??\n",
      "stl ?nola\n",
      "stockholm, sweden\n",
      "stockton on tees teesside uk\n",
      "storybrooke \n",
      "storybrooke / the moors\n",
      "stowmarket\n",
      "stratford, ct\n",
      "street of dallas\n",
      "studio\n",
      "subconscious la\n",
      "suburban detroit, michigan\n",
      "sugar land, tx\n",
      "sugarhouse, ut\n",
      "suginami-ku, tokyo, japan\n",
      "suitland\n",
      "sumter, sc\n",
      "sunbury, ohio\n",
      "sunny south florida \n",
      "sunnyvale, ca\n",
      "sunrise manor, nv\n",
      "sunshine coast, queensland\n",
      "suplex city\n",
      "surabaya\n",
      "surrey & manchester\n",
      "surrounded by weeaboos\n",
      "surry hills, sydney\n",
      "surulere lagos,home of swagg\n",
      "sutton, london uk\n",
      "suva, fiji islands.\n",
      "swag francisco\n",
      "swan river\n",
      "swaning around\n",
      "sweden\n",
      "swindon\n",
      "swindon,england \n",
      "swinfo@dot.state.al.us\n",
      "switzerland\n",
      "swmo\n",
      "sydney\n",
      "sydney & worldwide\n",
      "sydney australia\n",
      "sydney, australia\n",
      "sydney, new south wales\n",
      "sydney, nsw\n",
      "sylacauga, alabama\n",
      "sì£o luis\n",
      "sì£o paulo\n",
      "sì£o paulo sp,  brasil\n",
      "sì£o paulo, brasil\n",
      "t e x a s | wwat 8.24.14\n",
      "t-ville\n",
      "taco bell\n",
      "tacoma,washington\n",
      "tafekop ga-matsepe\n",
      "taken by piper curda\n",
      "taking bath do not disturb\n",
      "taking pain like pleasure\n",
      "tallahassee florida\n",
      "tallahassee, fl\n",
      "tama, iowa\n",
      "tampa\n",
      "tampa, fl\n",
      "tampa-st. petersburg, fl\n",
      "tamworth\n",
      "tarragona\n",
      "taylor swift\n",
      "team slytherin\n",
      "techfish \n",
      "ted&qz inc, ireland, europe\n",
      "teh internets\n",
      "telangana\n",
      "tema,accra\n",
      "temecula, ca\n",
      "temporary towers\n",
      "tennessee\n",
      "tennessee, usa\n",
      "tennessee/gallifrey\n",
      "terlingua, texas\n",
      "terre haute, in\n",
      "texas\n",
      "texas \n",
      "texas a&m university\n",
      "texas af\n",
      "texas, usa\n",
      "texas-usaã¢ ?\n",
      "texasss\n",
      "thailand\n",
      "thailand malaysia indonesia \n",
      "thane\n",
      "thanjavur\n",
      "the 5th dimension. \n",
      "the 6ix\n",
      "the ?? below ???\n",
      "the american wasteland (mv)\n",
      "the azure cloud\n",
      "the barn\n",
      "the burrow\n",
      "the canopy kingdom\n",
      "the circle of life\n",
      "the citadel, oldtown, westeros\n",
      "the d\n",
      "the dark\n",
      "the desert\n",
      "the desert of the real\n",
      "the dirty d\n",
      "the empire/first order\n",
      "the epicenter, and beyond\n",
      "the forever girl\n",
      "the globe\n",
      "the great state of maine \n",
      "the great state of texas\n",
      "the green and pleasant land.\n",
      "the grey area\n",
      "the hammock, fl, usa\n",
      "the harbinger.\n",
      "the howling\n",
      "the insane asylum. \n",
      "the internet\n",
      "the internet & nyc\n",
      "the internetz\n",
      "the jewfnited state\n",
      "the kingdom of fife, scotland\n",
      "the land of mass stupidity\n",
      "the land of new jersey. \n",
      "the local dump\n",
      "the low-cal calzone zone\n",
      "the main \n",
      "the meadow\n",
      "the memesphere\n",
      "the moon\n",
      "the multiverse\n",
      "the netherlands\n",
      "the north\n",
      "the orwellion police-state\n",
      "the own zone layer \n",
      "the p (south philly)\n",
      "the peach state\n",
      "the pig sty\n",
      "the pumpkin carriage of dreams\n",
      "the refrigerator \n",
      "the road to success\n",
      "the sanctuary network, rome\n",
      "the shady hyenatown of finland\n",
      "the shire\n",
      "the shores of lake kilby\n",
      "the south & westcoast \n",
      "the sun's corona\n",
      "the tardis\n",
      "the triskelion\n",
      "the uk\n",
      "the universe\n",
      "the void, u.s.a\n",
      "the waystone inn\n",
      "the web\n",
      "the weird part of wonderland\n",
      "the windy city\n",
      "the windy plains of denver\n",
      "the wood\n",
      "the world\n",
      "the world t.g.g / m.m.m \n",
      "they/her\n",
      "they/them\n",
      "they/them \n",
      "thibodaux, la\n",
      "third rock from the sun\n",
      "this is paradise. relax. \n",
      "thornton  colorado\n",
      "thrissur\n",
      "timaru district, new zealand\n",
      "timeline kamu\n",
      "tipperary (long way) \n",
      "tips on my blog at\n",
      "tn\n",
      "to the right of you!\n",
      "todaysbigstock.com\n",
      "tokyo\n",
      "tokyo & osaka\n",
      "toledo\n",
      "toledo, oh\n",
      "tonyj@centralizedhockey.com\n",
      "too far\n",
      "top secret\n",
      "top secret bunker \n",
      "topeka, ks\n",
      "tornado alley, usa \n",
      "toronto\n",
      "toronto û¢ dallas\n",
      "toronto, bob-lo, miami beach\n",
      "toronto, canada\n",
      "toronto, on\n",
      "toronto, on, canada\n",
      "toronto, ontario\n",
      "toronto, worldwide \n",
      "toronto-citizen of canada & us\n",
      "torrance, ca\n",
      "torry alvarez love forever ? ?\n",
      "trackside california\n",
      "tractor land aka bristol\n",
      "trancy manor\n",
      "trapped in america\n",
      "trapped in my conscience \n",
      "travelling around the world \n",
      "travelling to tae's pants\n",
      "traverse city, mi\n",
      "tri state\n",
      "tri-cities, wash.\n",
      "tring \n",
      "tring, uk\n",
      "trinidad & tobago\n",
      "trinidad and tobago\n",
      "trinity, bailiwick of jersey\n",
      "tripoli international airport\n",
      "tripsburg, ms.\n",
      "trost district\n",
      "trumann, arkansas\n",
      "tucson, arizona \n",
      "tucson, az\n",
      "tulalip, washington\n",
      "tulsa, ok\n",
      "tulsa, oklahoma\n",
      "tunbridge wells\n",
      "turkmenistan\n",
      "turner fenton\n",
      "tv5, philippines\n",
      "twitch.tv/naturalemblem26\n",
      "twitter lockout in progress\n",
      "twitterville\n",
      "two up two down\n",
      "tx\n",
      "tyler, tx\n",
      "tìáchira - venezuela\n",
      "u.k.\n",
      "u.s\n",
      "u.s.\n",
      "u.s. northern virginia\n",
      "u.s.a\n",
      "u.s.a and canada\n",
      "u.s.a.   fema region 5\n",
      "u.s.a. - global members site\n",
      "uae,sharjah/ abudhabi\n",
      "uga '15 alumnus - economics \n",
      "uganda\n",
      "uk\n",
      "uk  & germany\n",
      "uk & ibiza\n",
      "uk great britain \n",
      "uk, republic of ireland and australia\n",
      "uk,singer,songwriter,?2 act\n",
      "ukraine\n",
      "ukraine and ireland\n",
      "under santa barbara skies\n",
      "under the blanket\n",
      "unite. bless. wallahi \n",
      "united hoods of the globe\n",
      "united kingdom\n",
      "united kingdom,fraserburgh\n",
      "united states\n",
      "united states of america\n",
      "united states where it's warm\n",
      "unites states\n",
      "university heights, ohio\n",
      "university of chicago\n",
      "university of limerick\n",
      "university of south florida\n",
      "university of toronto\n",
      "unknown\n",
      "unknown \n",
      "unnamed city\n",
      "up a hill\n",
      "upper manhattan, new york\n",
      "upper st clair, pa\n",
      "uppsala, sweden\n",
      "upstairs.\n",
      "upstate new york\n",
      "uptown \n",
      "uruguay / westeros / gallifrey\n",
      "urì£nus\n",
      "us\n",
      "us, pa\n",
      "us-east-1a\n",
      "us-pr\n",
      "us: 44.414510,8.942499\n",
      "usa\n",
      "usa \n",
      "usa (formerly @usnoaagov)\n",
      "usa , az\n",
      "usa - canada - europe - asia\n",
      "usa, alabama\n",
      "usa, haiti, nepal\n",
      "usa, north dakota\n",
      "usa, wa\n",
      "usa/so florida via brooklyn ny\n",
      "usaov\n",
      "use #tmw in tweets get #rt\n",
      "utah\n",
      "utah, usa\n",
      "utica ny\n",
      "uyo, akwa ibom state, nigeria\n",
      "v-rp @ozrp_ ?mv, au, r18+?\n",
      "va\n",
      "va beach, virginia\n",
      "vail valley\n",
      "valle del sol\n",
      "valparaiso \n",
      "van buren, mo\n",
      "vancouver\n",
      "vancouver (hq) and worldwide\n",
      "vancouver bc\n",
      "vancouver canada\n",
      "vancouver usa\n",
      "vancouver, bc\n",
      "vancouver, bc, canada\n",
      "vancouver, bc.\n",
      "vancouver, british columbia\n",
      "vancouver, canada\n",
      "vancouver, colombie-britannique\n",
      "varanasi\n",
      "varies \n",
      "vcu\n",
      "venezuela\n",
      "ventura\n",
      "ventura, ca\n",
      "vermont, usa\n",
      "vero beach , fl\n",
      "very sw ca, usa....draenor\n",
      "victoria mozì£o \n",
      "victoria, australia, earth\n",
      "victoria, bc\n",
      "victoria, bc  canada\n",
      "victoria, british columbia\n",
      "victoria, canada\n",
      "victoria, tx.\n",
      "victorville, ca\n",
      "vidalia ga\n",
      "viejo\n",
      "vietnam\n",
      "vilnius\n",
      "vineyard\n",
      "virginia\n",
      "virginia, united states\n",
      "virginia, usa\n",
      "virgo supercluster\n",
      "visit my youtube channel.\n",
      "visit our  dedicated website @\n",
      "vista, ca\n",
      "viterbo bfa acting '18\n",
      "vitì_ria (es)\n",
      "vont island, lagos\n",
      "voorhees, nj\n",
      "vì_a lìáctea\n",
      "vì_sterì´s, sweden\n",
      "w. nykae \n",
      "w.i.t.s academy\n",
      "wa state\n",
      "waco tx\n",
      "waco, texas\n",
      "waddesdon\n",
      "wahpeton, nd\n",
      "waialua, hawaii\n",
      "wailuku, maui\n",
      "waistdeep, tx\n",
      "wakanda\n",
      "wakefield ma\n",
      "wakefield, west yorkshire\n",
      "wales\n",
      "wales, united kingdom\n",
      "walker county, alabama\n",
      "walking the tightrope\n",
      "walthamstow, london\n",
      "wanderlust\n",
      "wandsworth, london\n",
      "warm heart of africa\n",
      "warrandyte, australia\n",
      "warri\n",
      "warsaw\n",
      "warszawa\n",
      "warwick, ri @dollarocracy also\n",
      "washington\n",
      "washington d.c.\n",
      "washington dc\n",
      "washington dc / nantes, france\n",
      "washington state\n",
      "washington, d.c.\n",
      "washington, d.c. \n",
      "washington, d.c., area\n",
      "washington, dc\n",
      "washington, dc & charlotte, nc\n",
      "washington, dc 20009\n",
      "washington, dc native\n",
      "washington, krasnodar (russia)\n",
      "washington, usa\n",
      "washington,dc\n",
      "wasington, dc\n",
      "watch those videos -\n",
      "waterford mi\n",
      "waterfront\n",
      "waterloo, on\n",
      "waterloo, ont\n",
      "watertown, mass.\n",
      "watford\n",
      "waukesha, wi\n",
      "wausau, wisconsin\n",
      "waverly, ia\n",
      "we are global!\n",
      "we're all mad here\n",
      "we?it û¢ ixwin\n",
      "webster, tx\n",
      "wellington\n",
      "wellington, new zealand\n",
      "welt\n",
      "wema building\n",
      "west\n",
      "west africa\n",
      "west bank, gaza strip\n",
      "west chester, pa\n",
      "west coast, cali usa\n",
      "west coast, usa\n",
      "west hollywood\n",
      "west hollywood, ca\n",
      "west lancashire, uk.\n",
      "west midlands\n",
      "west palm beach, florida\n",
      "west richland, wa\n",
      "west vancouver, b.c.\n",
      "west virginia, usa\n",
      "west wales\n",
      "westchester\n",
      "westend, puritan ave \n",
      "westerland\n",
      "western new york\n",
      "western washington\n",
      "weston super mare\n",
      "westside of philly 7? block??\n",
      "westwestwestwestwestwestwest\n",
      "weyburn\n",
      "where ever i please\n",
      "where i need to be\n",
      "where i'm supposed to be\n",
      "where the money at\n",
      "where the wild things are\n",
      "wherever i'm needed\n",
      "wherever i'm sent\n",
      "wherever the $$$ at\n",
      "wherever there's netflix\n",
      "wherever-the-fuck washington\n",
      "whippany, nj\n",
      "whitby, on\n",
      "white plains, ny\n",
      "whiterun, skyrim\n",
      "whole world \n",
      "whs '17\n",
      "why should you know?\n",
      "wilbraham, ma\n",
      "wild wild web\n",
      "wildomar, ca\n",
      "williamsbridge, bronx, new yor\n",
      "williamsburg, va\n",
      "williamstown, vt\n",
      "wilmington, de\n",
      "wilmington, delaware\n",
      "wilmington, nc\n",
      "wiltshire\n",
      "windsor,ontario\n",
      "winnipeg\n",
      "winnipeg, manitoba\n",
      "winnipeg, mb, canada\n",
      "winston salem, north carolina\n",
      "winston-salem north carolina\n",
      "winston-salem, nc\n",
      "winter park, colorado\n",
      "wisco\n",
      "wisconsin\n",
      "wisconsin, usa\n",
      "with doflamingo\n",
      "wny\n",
      "wolmers trust school for boys \n",
      "wolverhampton\n",
      "wolverhampton/brum/jersey\n",
      "wonderlandûó ?????? ???? ??????\n",
      "wood buffalo, alberta\n",
      "woodcreek hs, roseville, ca\n",
      "woosley\n",
      "worcester, ma\n",
      "wordldwide\n",
      "world\n",
      "world news\n",
      "world wide\n",
      "world wide web\n",
      "world wide!!\n",
      "worldwi$e \n",
      "worldwide\n",
      "worldwide - global\n",
      "worldwide!\n",
      "worldwide-boston\n",
      "worldwide.\n",
      "worldwideweb\n",
      "wrapped arnd hyuk's finger\n",
      "wrex\n",
      "wrigley field\n",
      "wv, love the blue and gold\n",
      "www.aprylpooley.com\n",
      "www.facebook.com/stuntfm\n",
      "www.tmgcgart.com\n",
      "www.twitch.tv/pksparkxx\n",
      "www.youtube.com?malkavius2\n",
      "wynne, ar\n",
      "wyoming, mi (grand rapids)\n",
      "xi'an, china\n",
      "xiumin's nonexistent solos\n",
      "y(our) boyfriends legs \n",
      "y/e/l\n",
      "ya motha bed\n",
      "yadkinville, nc\n",
      "yamaku academy, class 3-4\n",
      "yeezy taught me , nv\n",
      "yellowknife\n",
      "yellowknife, nt\n",
      "yewa zone\n",
      "ylisse\n",
      "yobe state\n",
      "yogya berhati nyaman\n",
      "yooooooo\n",
      "yorkshire\n",
      "\n",
      "you're not 19 forever   \n",
      "youngstown, oh\n",
      "your conversation\n",
      "your notifications\n",
      "your screen\n",
      "your sister's bedroom\n",
      "your six\n",
      "youtube.com/channel/uchwtlc9b4zjugh7ydlb55iw\n",
      "yuba city, ca\n",
      "yulee, fl\n",
      "yuuko-san's shop\n",
      "zac newsome loves me\n",
      "zboyer@washingtontimes.com\n",
      "zeerust, south africa\n",
      "zero branco\n",
      "ziam af \n",
      "zimbabwe\n",
      "{detailed}\n",
      "{got | modern au | lizz}\n",
      "| ca û¢ ga  |\n",
      "|-/\n",
      "|| c h i c a g o ||\n",
      "êwagger!ìominicanì÷\n",
      "û¢ views from the six û¢\n",
      "û¢5û¢12û¢14û¢ | åè#savioursquadåç\n",
      "û¢901û¢\n",
      "û¢flgû¢\n",
      "û¢iii.xii.mmxiû¢\n",
      "û¢oldercandybloomû¢\n",
      "ûêûêûê\n",
      "å_: ?? ìñ ? : ?\n",
      "å_å_los mina cityã¢\n",
      "å¡å¡midwest û¢û¢\n",
      "åê(?û¢`?û¢å«)??\n",
      "åø\\_(?)_/åø\n",
      "ìït: -26.695807,27.837865\n",
      "ìït: 0.0,0.0\n",
      "ìït: 1.50225,103.742992\n",
      "ìït: 10.614817868480726,12.195582811791382\n",
      "ìït: 19.123127,72.825133\n",
      "ìït: 27.9136024,-81.6078532\n",
      "ìït: 30.307558,-81.403118\n",
      "ìït: 33.209923,-87.545328\n",
      "ìït: 35.223347,-80.827834\n",
      "ìït: 36.142163,-95.979189\n",
      "ìït: 39.982988,-75.261624\n",
      "ìït: 40.562796,-75.488849\n",
      "ìït: 40.707762,-74.014213\n",
      "ìït: 41.252426,-96.072013\n",
      "ìït: 42.910975,-78.865828\n",
      "ìït: 43.631838,-79.55807\n",
      "ìït: 6.4682,3.18287\n",
      "ìït: 6.488400524109015,3.352798039832285\n",
      "ìøåàå_t: 40.736324,-73.990062\n"
     ]
    }
   ],
   "source": [
    "for loc in loc_group.groups.keys():\n",
    "    print(loc)"
   ]
  },
  {
   "source": [
    "It appears that a lot of Twitter users write locations that are badly formatted, jokes or simply non-informative. Even those who put a real location don't follow any standard: some write only country, while others write city or US state.\n",
    "\n",
    "We could try to clean up this feature and use it, but it would be non-trivial and with no guaranteed results. We'll leave it be and use only text data for now."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Text Preprocessing\n",
    "To preprocess the text feature, we will remove HTML tags with beautiful soup and remove all characters except alphanumeric and hashtags.\n",
    "Then, we'll split the text into words and keep only their radicals, using nltk PorterStemmer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\olavo\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and clean text features\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = str(text)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9#]\", \" \", text) # Keep only alphanumeric characters and hashtags\n",
    "    text = text.lower()\n",
    "    words = set(text.split()) # Split string into words\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = list(words - stopwords_set)\n",
    "    \n",
    "    # Stem words with PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    def stem_word(x):\n",
    "        return stemmer.stem(x)\n",
    "    words = list(map(stem_word, words))\n",
    "    \n",
    "    return words\n",
    "\n",
    " \n",
    "# def process_dataset(dataset, cached_path=Path(commons.dataset_path) / 'train_processed.csv'):\n",
    "#     '''Apply text preprocessing and BoW codification to the entire dataset'''\n",
    "#     cached_path = Path(cached_path)\n",
    "#     if cached_path.is_file():\n",
    "#         return pd.read_csv(cached_path)\n",
    "    \n",
    "#     dataset['text_proc'] = dataset['text'].apply(process_text)\n",
    "#     if cached_path:\n",
    "#         dataset.to_csv(cached_path)\n",
    "#     return dataset"
   ]
  },
  {
   "source": [
    "We can see how each text is split into word radicals below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['allah', 'reason', 'may', 'forgiv', '#earthquak', 'us', 'deed']\n['forest', 'la', 'near', 'rong', 'fire', 'canada', 'sask']\n['ask', 'place', 'resid', 'expect', 'shelter', 'evacu', 'notifi', 'order', 'offic']\n['california', '000', 'peopl', '13', 'receiv', 'evacu', '#wildfir', 'order']\n['smoke', 'sent', 'photo', '#alaska', 'rubi', 'got', '#wildfir', 'school', 'pour']\n['close', 'due', 'fire', 'california', 'hwi', 'updat', 'counti', '#rockyfir', '20', '#wildfir', '#cafir', 'direct', 'lake']\n['caus', '#disast', '#flood', 'flash', 'street', 'colorado', 'heavi', 'manit', 'rain', 'spring', 'flood', 'area']\n['hill', 'see', 'fire', 'wood', 'top']\n['emerg', 'build', 'across', 'evacu', 'happen', 'street']\n['come', 'afraid', 'area', 'tornado']\n['three', 'die', 'far', 'wave', 'peopl', 'heat']\n['tampa', 'second', 'flood', 'live', 'hah', 'gonna', 'south', 'get', '#flood', 'wait', 'fvck', 'haha']\n['#tampabay', '#rain', 'day', '#florida', '#flood', '19', '18', '#tampa', 'lost', 'count']\n['bago', '#flood', 'arriv', 'myanmar', '#we']\n['#break', 'car', 'crash', 'bu', '80', 'school', 'damag', 'multi']\n['man']\n['love', 'fruit']\n['summer', 'love']\n['car', 'fast']\n['goooooooaaaaaal']\n['ridicul']\n"
     ]
    }
   ],
   "source": [
    "for entry in train_proc.loc[0:20, 'text']:\n",
    "    print(process_text(entry))"
   ]
  },
  {
   "source": [
    "Then, we'll process the entire dataset using the previous function and sklearn CountVectorizer to convert the processed texts into a Bag of Words representation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Cleaning text...\n",
      "\n",
      "Assembling Bag of Words matrix...\n",
      "\n",
      "Splitting dataset...\n",
      "\n",
      "Saving dataset to file...\n"
     ]
    }
   ],
   "source": [
    "from libs.dataset import create_dataset\n",
    "\n",
    "seed                 = 10\n",
    "train_path           = Path(commons.dataset_path) / \"train.csv\"\n",
    "test_path            = Path(commons.dataset_path) / \"test.csv\"\n",
    "\n",
    "# Create train and validation datasets and save to file\n",
    "train_x, val_x, train_y, val_y = create_dataset(train_path, test_path, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      #  #1  #360wisenew  #7  #9  #abstorm  #accid  #africa  #afterlif  \\\n",
       "2572  0   0            0   0   0         0       0        0          0   \n",
       "1813  0   0            0   0   0         0       0        0          0   \n",
       "2767  0   0            0   0   0         0       0        0          0   \n",
       "6248  0   0            0   0   0         0       0        0          0   \n",
       "7563  0   0            0   0   0         0       0        0          0   \n",
       "...  ..  ..          ...  ..  ..       ...     ...      ...        ...   \n",
       "3441  0   0            0   0   0         0       0        0          0   \n",
       "1344  0   0            0   0   0         0       0        0          0   \n",
       "4623  0   0            0   0   0         0       0        0          0   \n",
       "7293  0   0            0   0   0         0       0        0          0   \n",
       "1289  0   0            0   0   0         0       0        0          0   \n",
       "\n",
       "      #airplan  ...  yr  z  z10  zak  zayn  zero  zombi  zone  zouma  \\\n",
       "2572         0  ...   0  0    0    0     0     0      0     0      0   \n",
       "1813         0  ...   0  0    0    0     0     0      0     0      0   \n",
       "2767         0  ...   0  0    0    0     0     0      0     0      0   \n",
       "6248         0  ...   0  0    0    0     0     0      0     0      0   \n",
       "7563         0  ...   0  0    0    0     0     0      0     0      0   \n",
       "...        ...  ...  .. ..  ...  ...   ...   ...    ...   ...    ...   \n",
       "3441         0  ...   0  0    0    0     0     0      0     0      0   \n",
       "1344         0  ...   0  0    0    0     0     0      0     0      0   \n",
       "4623         0  ...   0  0    0    0     0     0      0     0      0   \n",
       "7293         0  ...   0  0    0    0     0     0      0     0      0   \n",
       "1289         0  ...   0  0    0    0     0     0      0     0      0   \n",
       "\n",
       "      zujwuiomb3  \n",
       "2572           0  \n",
       "1813           0  \n",
       "2767           0  \n",
       "6248           0  \n",
       "7563           0  \n",
       "...          ...  \n",
       "3441           0  \n",
       "1344           0  \n",
       "4623           0  \n",
       "7293           0  \n",
       "1289           0  \n",
       "\n",
       "[6090 rows x 5000 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>#</th>\n      <th>#1</th>\n      <th>#360wisenew</th>\n      <th>#7</th>\n      <th>#9</th>\n      <th>#abstorm</th>\n      <th>#accid</th>\n      <th>#africa</th>\n      <th>#afterlif</th>\n      <th>#airplan</th>\n      <th>...</th>\n      <th>yr</th>\n      <th>z</th>\n      <th>z10</th>\n      <th>zak</th>\n      <th>zayn</th>\n      <th>zero</th>\n      <th>zombi</th>\n      <th>zone</th>\n      <th>zouma</th>\n      <th>zujwuiomb3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2572</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1813</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2767</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6248</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7563</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3441</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1344</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4623</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7293</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1289</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6090 rows × 5000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.dataset import TextDataset\n",
    "\n",
    "val_processed_path   = Path(commons.dataset_path) / \"val_processed.csv\"\n",
    "val_dataloader = TextDataset(val_processed_path, target_column=commons.target_column_name,\n",
    "        normalize=True, balance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             #        #1  #360wisenew   #7        #9  #abstorm  #accid  \\\n",
       "0    -0.085294 -0.044426    -0.036262  0.0 -0.036262 -0.036262     0.0   \n",
       "1    -0.085294 -0.044426    -0.036262  0.0 -0.036262 -0.036262     0.0   \n",
       "2    -0.085294 -0.044426    -0.036262  0.0 -0.036262 -0.036262     0.0   \n",
       "3    -0.085294 -0.044426    -0.036262  0.0 -0.036262 -0.036262     0.0   \n",
       "4    -0.085294 -0.044426    -0.036262  0.0 -0.036262 -0.036262     0.0   \n",
       "...        ...       ...          ...  ...       ...       ...     ...   \n",
       "1518 -0.085294 -0.044426    -0.036262  0.0 -0.036262 -0.036262     0.0   \n",
       "1519 -0.085294 -0.044426    -0.036262  0.0 -0.036262 -0.036262     0.0   \n",
       "1520 -0.085294 -0.044426    -0.036262  0.0 -0.036262 -0.036262     0.0   \n",
       "1521 -0.085294 -0.044426    -0.036262  0.0 -0.036262 -0.036262     0.0   \n",
       "1522 -0.085294 -0.044426    -0.036262  0.0 -0.036262 -0.036262     0.0   \n",
       "\n",
       "       #africa  #afterlif  #airplan  ...        yr    z  z10  zak  zayn  \\\n",
       "0    -0.025633  -0.025633       0.0  ... -0.025633  0.0  0.0  0.0   0.0   \n",
       "1    -0.025633  -0.025633       0.0  ... -0.025633  0.0  0.0  0.0   0.0   \n",
       "2    -0.025633  -0.025633       0.0  ... -0.025633  0.0  0.0  0.0   0.0   \n",
       "3    -0.025633  -0.025633       0.0  ... -0.025633  0.0  0.0  0.0   0.0   \n",
       "4    -0.025633  -0.025633       0.0  ... -0.025633  0.0  0.0  0.0   0.0   \n",
       "...        ...        ...       ...  ...       ...  ...  ...  ...   ...   \n",
       "1518 -0.025633  -0.025633       0.0  ... -0.025633  0.0  0.0  0.0   0.0   \n",
       "1519 -0.025633  -0.025633       0.0  ... -0.025633  0.0  0.0  0.0   0.0   \n",
       "1520 -0.025633  -0.025633       0.0  ... -0.025633  0.0  0.0  0.0   0.0   \n",
       "1521 -0.025633  -0.025633       0.0  ... -0.025633  0.0  0.0  0.0   0.0   \n",
       "1522 -0.025633  -0.025633       0.0  ... -0.025633  0.0  0.0  0.0   0.0   \n",
       "\n",
       "           zero  zombi      zone  zouma  zujwuiomb3  \n",
       "0     39.012818    0.0 -0.077101    0.0         0.0  \n",
       "1     -0.025633    0.0 -0.077101    0.0         0.0  \n",
       "2     -0.025633    0.0 -0.077101    0.0         0.0  \n",
       "3     -0.025633    0.0 -0.077101    0.0         0.0  \n",
       "4     -0.025633    0.0 -0.077101    0.0         0.0  \n",
       "...         ...    ...       ...    ...         ...  \n",
       "1518  -0.025633    0.0 -0.077101    0.0         0.0  \n",
       "1519  -0.025633    0.0 -0.077101    0.0         0.0  \n",
       "1520  -0.025633    0.0 -0.077101    0.0         0.0  \n",
       "1521  -0.025633    0.0 -0.077101    0.0         0.0  \n",
       "1522  -0.025633    0.0 -0.077101    0.0         0.0  \n",
       "\n",
       "[1523 rows x 5000 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>#</th>\n      <th>#1</th>\n      <th>#360wisenew</th>\n      <th>#7</th>\n      <th>#9</th>\n      <th>#abstorm</th>\n      <th>#accid</th>\n      <th>#africa</th>\n      <th>#afterlif</th>\n      <th>#airplan</th>\n      <th>...</th>\n      <th>yr</th>\n      <th>z</th>\n      <th>z10</th>\n      <th>zak</th>\n      <th>zayn</th>\n      <th>zero</th>\n      <th>zombi</th>\n      <th>zone</th>\n      <th>zouma</th>\n      <th>zujwuiomb3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.085294</td>\n      <td>-0.044426</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.036262</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>39.012818</td>\n      <td>0.0</td>\n      <td>-0.077101</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.085294</td>\n      <td>-0.044426</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.036262</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>-0.077101</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.085294</td>\n      <td>-0.044426</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.036262</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>-0.077101</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.085294</td>\n      <td>-0.044426</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.036262</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>-0.077101</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.085294</td>\n      <td>-0.044426</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.036262</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>-0.077101</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1518</th>\n      <td>-0.085294</td>\n      <td>-0.044426</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.036262</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>-0.077101</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1519</th>\n      <td>-0.085294</td>\n      <td>-0.044426</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.036262</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>-0.077101</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1520</th>\n      <td>-0.085294</td>\n      <td>-0.044426</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.036262</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>-0.077101</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1521</th>\n      <td>-0.085294</td>\n      <td>-0.044426</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.036262</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>-0.077101</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1522</th>\n      <td>-0.085294</td>\n      <td>-0.044426</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.036262</td>\n      <td>-0.036262</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.025633</td>\n      <td>0.0</td>\n      <td>-0.077101</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1523 rows × 5000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "val_dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "#              False\n",
       "#1             False\n",
       "#360wisenew    False\n",
       "#7              True\n",
       "#9             False\n",
       "               ...  \n",
       "zero           False\n",
       "zombi           True\n",
       "zone           False\n",
       "zouma           True\n",
       "zujwuiomb3      True\n",
       "Length: 5000, dtype: bool"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "val_dataloader.dataset.std(axis=0) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['#7', '#accid', '#airplan', '#animalrescu', '#antioch', '#artistsunit',\n       '#arwx', '#atlanta', '#avalanch', '#bb17',\n       ...\n       'yell', 'yemen', 'yorker', 'z', 'z10', 'zak', 'zayn', 'zombi', 'zouma',\n       'zujwuiomb3'],\n      dtype='object', length=1702)\n"
     ]
    }
   ],
   "source": [
    "print(val_dataloader.dataset.columns[val_dataloader.dataset.std(axis=0) == 0])\n",
    "# val_dataloader.dataset.drop(columns=val_dataloader.dataset.std(axis=0) == 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}